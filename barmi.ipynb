{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcd2bda-2002-4d57-8529-6029457359dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0439aa-93d5-4982-af6a-f7efde6fa283",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.8/site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Downloading pip-22.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-22.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers[torch]\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.2.1-py3-none-any.whl (342 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.2/342.2 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.3.5)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.22.2)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gensim\n",
      "  Downloading gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.8.0-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ray[tune]\n",
      "  Downloading ray-1.12.1-cp38-cp38-manylinux2014_x86_64.whl (52.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.9/52.9 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting optuna\n",
      "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.2/308.2 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers[torch]->-r requirements.txt (line 1)) (4.62.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers[torch]->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers[torch]->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers[torch]->-r requirements.txt (line 1)) (2.26.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers[torch]->-r requirements.txt (line 1)) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m149.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers[torch]->-r requirements.txt (line 1)) (2022.1.18)\n",
      "Requirement already satisfied: torch>=1.0 in /opt/conda/lib/python3.8/site-packages (from transformers[torch]->-r requirements.txt (line 1)) (1.11.0a0+17540c5)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 2)) (2022.1.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.3/128.3 kB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.1/212.1 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 3)) (2021.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from sklearn->-r requirements.txt (line 5)) (0.24.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.8/site-packages (from gensim->-r requirements.txt (line 6)) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.8/site-packages (from gensim->-r requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.8/site-packages (from ray[tune]->-r requirements.txt (line 8)) (4.4.0)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /opt/conda/lib/python3.8/site-packages (from ray[tune]->-r requirements.txt (line 8)) (3.19.4)\n",
      "Collecting virtualenv\n",
      "  Downloading virtualenv-20.14.1-py2.py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.8/site-packages (from ray[tune]->-r requirements.txt (line 8)) (8.0.3)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.8/site-packages (from ray[tune]->-r requirements.txt (line 8)) (18.2.0)\n",
      "Collecting frozenlist\n",
      "  Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.7/158.7 kB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ray[tune]->-r requirements.txt (line 8)) (1.0.3)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /opt/conda/lib/python3.8/site-packages (from ray[tune]->-r requirements.txt (line 8)) (1.43.0)\n",
      "Collecting aiosignal\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting tensorboardX>=1.9\n",
      "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tabulate in /opt/conda/lib/python3.8/site-packages (from ray[tune]->-r requirements.txt (line 8)) (0.8.9)\n",
      "Collecting sqlalchemy>=1.1.0\n",
      "  Downloading SQLAlchemy-1.4.36-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cliff\n",
      "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m150.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting colorlog\n",
      "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.8/site-packages (from grpcio<=1.43.0,>=1.28.1->ray[tune]->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[torch]->-r requirements.txt (line 1)) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers[torch]->-r requirements.txt (line 1)) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers[torch]->-r requirements.txt (line 1)) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers[torch]->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers[torch]->-r requirements.txt (line 1)) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers[torch]->-r requirements.txt (line 1)) (1.26.7)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.6/156.6 kB\u001b[0m \u001b[31m144.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.2)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.8/site-packages (from alembic->optuna->-r requirements.txt (line 9)) (5.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.8/site-packages (from alembic->optuna->-r requirements.txt (line 9)) (4.11.1)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.9.0-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting autopage>=0.4.0\n",
      "  Downloading autopage-0.5.0-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.8/site-packages (from cliff->optuna->-r requirements.txt (line 9)) (3.1.0)\n",
      "Collecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cmd2>=1.0.0\n",
      "  Downloading cmd2-2.4.1-py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema->ray[tune]->-r requirements.txt (line 8)) (0.18.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 5)) (1.1.0)\n",
      "Collecting distlib<1,>=0.3.1\n",
      "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.2/461.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs<3,>=2 in /opt/conda/lib/python3.8/site-packages (from virtualenv->ray[tune]->-r requirements.txt (line 8)) (2.4.1)\n",
      "Collecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna->-r requirements.txt (line 9)) (0.2.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources->alembic->optuna->-r requirements.txt (line 9)) (3.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.8/site-packages (from Mako->alembic->optuna->-r requirements.txt (line 9)) (2.0.1)\n",
      "Building wheels for collected packages: sklearn, pyperclip\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=1e420694e3f374645bb883fd12afa8246e5344113ed7397e74eba843cc4f9268\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4wnskc_6/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=e77eb37b3f4d7d9741dfe7ac2bfc668c7f14556a9ff8559c5c47b1274710b885\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4wnskc_6/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
      "Successfully built sklearn pyperclip\n",
      "Installing collected packages: tokenizers, pyperclip, distlib, xxhash, virtualenv, pbr, numpy, Mako, greenlet, frozenlist, dill, colorlog, cmd2, autopage, async-timeout, tensorboardX, stevedore, sqlalchemy, responses, pyarrow, pandas, multiprocess, huggingface-hub, cmaes, aiosignal, accelerate, transformers, ray, gensim, cliff, alembic, aiohttp, sklearn, optuna, datasets\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.2\n",
      "    Uninstalling numpy-1.22.2:\n",
      "      Successfully uninstalled numpy-1.22.2\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 5.0.0\n",
      "    Uninstalling pyarrow-5.0.0:\n",
      "      Successfully uninstalled pyarrow-5.0.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 21.12.0a0+293.g0930f712e6 requires pandas<1.4.0dev0,>=1.0, but you have pandas 1.4.2 which is incompatible.\n",
      "cudf 21.12.0a0+293.g0930f712e6 requires pandas<1.4.0dev0,>=1.0, but you have pandas 1.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Mako-1.2.0 accelerate-0.8.0 aiohttp-3.8.1 aiosignal-1.2.0 alembic-1.7.7 async-timeout-4.0.2 autopage-0.5.0 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.1 colorlog-6.6.0 datasets-2.2.1 dill-0.3.4 distlib-0.3.4 frozenlist-1.3.0 gensim-4.2.0 greenlet-1.1.2 huggingface-hub-0.6.0 multiprocess-0.70.12.2 numpy-1.22.3 optuna-2.10.0 pandas-1.4.2 pbr-5.9.0 pyarrow-8.0.0 pyperclip-1.8.2 ray-1.12.1 responses-0.18.0 sklearn-0.0 sqlalchemy-1.4.36 stevedore-3.5.0 tensorboardX-2.5 tokenizers-0.12.1 transformers-4.19.2 virtualenv-20.14.1 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.8/site-packages (0.8.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from accelerate) (1.11.0a0+17540c5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from accelerate) (1.22.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.4.0->accelerate) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U -r requirements.txt\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe80cadd-f415-44aa-9afe-5dca115ffb07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb0ca4161114ed7be7087942f7d3465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import datasets\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import argparse\n",
    "from huforpred.bert.utils import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ea8774-f4a5-4b2a-bfdb-6d0385dcfd25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0023b6ca1bb4b7d6\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/default-0023b6ca1bb4b7d6/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827df99aa5534b6594341e64c9ac4dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data_path = Path('/datasets/finhubert_filter')\n",
    "\n",
    "labeled_data = datasets.Dataset.from_parquet(\n",
    "    {\n",
    "        \"train\": (training_data_path / \"labelled_train_gyenes.parquet\").as_posix(),\n",
    "        \"validation\": (training_data_path / \"labelled_test_gyenes.parquet\").as_posix(),\n",
    "        \"test\": (training_data_path / \"labelled_test.parquet\").as_posix(),\n",
    "    },\n",
    "    features=datasets.Features(\n",
    "        {\"text\": datasets.Value(\"string\"), \"label\": datasets.ClassLabel(num_classes=2)}\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10255f53-edd2-4bac-8462-12fddd5f14c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/papsebestyen/hubert-base-cc-finetuned-forum/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b16f0a18c599b551464c586c0b96b1a978cd8c84de9527cb43a5de35d90290e7.dd0380e6ef41dcfd523f8aa0fff30e0a3cd32404089a9e177004c7af42b35735\n",
      "loading file https://huggingface.co/papsebestyen/hubert-base-cc-finetuned-forum/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/dbb51c7fc96d99548e4ebda2556e2dccdc396a5cce93eeba4fa189e0e191af77.828a0f4339e10fc1b77efae710889bcc2d75d7c49e48d4d5c6ff945ed96c27b9\n",
      "loading file https://huggingface.co/papsebestyen/hubert-base-cc-finetuned-forum/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/papsebestyen/hubert-base-cc-finetuned-forum/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/a2c12af4e743fb6398dab8d983db0dc668f5ddad8d4f53f56cb7cdc51dc21fbb.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/papsebestyen/hubert-base-cc-finetuned-forum/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/e67e2e5ad0da1322df1cac7a88c3a396d82d2b8824bdfdc580fa033518efebdf.c2689cbdac0b3ee8705be3ef81c7f731fb91bbaa462a84abf219695310c1b737\n",
      "loading configuration file https://huggingface.co/papsebestyen/hubert-base-cc-finetuned-forum/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f5cd345e418d68e169edc61c5844c0cd79b365390d8678378d02f320daea274d.04d86289ede761646c08e85eca9f9a3175941fd009cef773f74721d0fdcec0fb\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"SZTAKI-HLT/hubert-base-cc\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/papsebestyen/hubert-base-cc-finetuned-forum/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/eccea84c04bc247ab7a590722b757628d8ac3bac079eea25373aa1857c9356e6.aab0aa52f5f6508db03351efa12981eafb250cad34f7be6d35142a6c3810dded\n",
      "Some weights of the model checkpoint at papsebestyen/hubert-base-cc-finetuned-forum were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at papsebestyen/hubert-base-cc-finetuned-forum and are newly initialized: ['bert.pooler.dense.weight', 'classifier.bias', 'bert.pooler.dense.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/default-0023b6ca1bb4b7d6/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-2704233ad506565f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/default-0023b6ca1bb4b7d6/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-5d9a2a335f79a6ed.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/default-0023b6ca1bb4b7d6/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-32f5b9632dae3425.arrow\n"
     ]
    }
   ],
   "source": [
    "model_name = \"papsebestyen/hubert-base-cc-finetuned-forum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,\n",
    "    #attention_probs_dropout_prob=0.3,\n",
    "    #hidden_dropout_prob=0.3,\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_data = labeled_data.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd6a2da-212f-4b90-afee-d1118bf579cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c016fe8b-fda7-45e3-a9e3-ddc8add4dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_config = json.loads(Path('best_params_gyenes_v1.json').read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ace3bf-56cf-41f8-9197-44ead4720276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44059d6fd5ff4ce5a9de68f2c7b739fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdf6a0ad-39fa-44d1-a149-3c8c832e0e47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git version 2.25.1\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (2.9.2-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n",
      "Updated git hooks.\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "!git --version\n",
    "!apt-get install git-lfs\n",
    "!git-lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1924065c-3819-4dfc-89f6-633437a27999",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/notebooks/hubert-base-cc-finance-filter is already a clone of https://huggingface.co/papsebestyen/hubert-base-cc-finance-filter. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(tokenized_data[\"train\"]) // batch_size\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='hubert-base-cc-finance-filter',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    push_to_hub=True,\n",
    "    logging_steps = logging_steps,\n",
    "    auto_find_batch_size = True,\n",
    "    fp16=True,\n",
    "    **tuned_config\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0d0194c-3cd0-4b8f-a358-bffc0ff63800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 60\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 60\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 70\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 60\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 60\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 135\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 60\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 60\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 270\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 23:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.571700</td>\n",
       "      <td>0.691788</td>\n",
       "      <td>0.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.410400</td>\n",
       "      <td>0.423628</td>\n",
       "      <td>0.711864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.600147</td>\n",
       "      <td>0.745098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>0.538848</td>\n",
       "      <td>0.767123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.538848</td>\n",
       "      <td>0.767123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to hubert-base-cc-finance-filter/checkpoint-54\n",
      "Configuration saved in hubert-base-cc-finance-filter/checkpoint-54/config.json\n",
      "Model weights saved in hubert-base-cc-finance-filter/checkpoint-54/pytorch_model.bin\n",
      "tokenizer config file saved in hubert-base-cc-finance-filter/checkpoint-54/tokenizer_config.json\n",
      "Special tokens file saved in hubert-base-cc-finance-filter/checkpoint-54/special_tokens_map.json\n",
      "tokenizer config file saved in hubert-base-cc-finance-filter/tokenizer_config.json\n",
      "Special tokens file saved in hubert-base-cc-finance-filter/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to hubert-base-cc-finance-filter/checkpoint-108\n",
      "Configuration saved in hubert-base-cc-finance-filter/checkpoint-108/config.json\n",
      "Model weights saved in hubert-base-cc-finance-filter/checkpoint-108/pytorch_model.bin\n",
      "tokenizer config file saved in hubert-base-cc-finance-filter/checkpoint-108/tokenizer_config.json\n",
      "Special tokens file saved in hubert-base-cc-finance-filter/checkpoint-108/special_tokens_map.json\n",
      "tokenizer config file saved in hubert-base-cc-finance-filter/tokenizer_config.json\n",
      "Special tokens file saved in hubert-base-cc-finance-filter/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to hubert-base-cc-finance-filter/checkpoint-162\n",
      "Configuration saved in hubert-base-cc-finance-filter/checkpoint-162/config.json\n",
      "Model weights saved in hubert-base-cc-finance-filter/checkpoint-162/pytorch_model.bin\n",
      "tokenizer config file saved in hubert-base-cc-finance-filter/checkpoint-162/tokenizer_config.json\n",
      "Special tokens file saved in hubert-base-cc-finance-filter/checkpoint-162/special_tokens_map.json\n",
      "tokenizer config file saved in hubert-base-cc-finance-filter/tokenizer_config.json\n",
      "Special tokens file saved in hubert-base-cc-finance-filter/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to hubert-base-cc-finance-filter/checkpoint-216\n",
      "Configuration saved in hubert-base-cc-finance-filter/checkpoint-216/config.json\n",
      "Model weights saved in hubert-base-cc-finance-filter/checkpoint-216/pytorch_model.bin\n",
      "tokenizer config file saved in hubert-base-cc-finance-filter/checkpoint-216/tokenizer_config.json\n",
      "Special tokens file saved in hubert-base-cc-finance-filter/checkpoint-216/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 400\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to hubert-base-cc-finance-filter/checkpoint-270\n",
      "Configuration saved in hubert-base-cc-finance-filter/checkpoint-270/config.json\n",
      "Model weights saved in hubert-base-cc-finance-filter/checkpoint-270/pytorch_model.bin\n",
      "tokenizer config file saved in hubert-base-cc-finance-filter/checkpoint-270/tokenizer_config.json\n",
      "Special tokens file saved in hubert-base-cc-finance-filter/checkpoint-270/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from hubert-base-cc-finance-filter/checkpoint-216 (score: 0.767123287671233).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=270, training_loss=0.3162301103274027, metrics={'train_runtime': 1436.3011, 'train_samples_per_second': 2.785, 'train_steps_per_second': 0.188, 'total_flos': 1052444221440000.0, 'train_loss': 0.3162301103274027, 'epoch': 5.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c62d64d-1e05-437e-bfeb-32c5a069160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to hubert-base-cc-finance-filter\n",
      "Configuration saved in hubert-base-cc-finance-filter/config.json\n",
      "Model weights saved in hubert-base-cc-finance-filter/pytorch_model.bin\n",
      "tokenizer config file saved in hubert-base-cc-finance-filter/tokenizer_config.json\n",
      "Special tokens file saved in hubert-base-cc-finance-filter/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c41a4aef614b1a86acf7de5280b5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/422M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9584605d129f443da46b0ac2e8ddcae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/May19_18-59-01_ne49ze6z08/events.out.tfevents.1652986748.ne49ze6z08.10943.4: 100%|##########|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Enforcing permissions...        \n",
      "remote: Allowed refs: all        \n",
      "To https://huggingface.co/papsebestyen/hubert-base-cc-finance-filter\n",
      "   696fbe5..eeff528  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'F1', 'type': 'f1', 'value': 0.767123287671233}]}\n",
      "remote: Enforcing permissions...        \n",
      "remote: Allowed refs: all        \n",
      "To https://huggingface.co/papsebestyen/hubert-base-cc-finance-filter\n",
      "   eeff528..05818d1  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/papsebestyen/hubert-base-cc-finance-filter/commit/eeff528232829eac92a9b26fc6eb63fcc3ce4a14'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "032449a4-1ce8-4eb3-8f02-233679be1317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 400\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e8a64b9-5284-442a-8f4c-e0af6782ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00904ad4-9b41-40e3-a182-2b1ee34beab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "095221f7-72c0-4080-9b24-8f35ac944484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.92827   0.81181   0.86614       271\n",
      "           1    0.68712   0.86822   0.76712       129\n",
      "\n",
      "    accuracy                        0.83000       400\n",
      "   macro avg    0.80769   0.84001   0.81663       400\n",
      "weighted avg    0.85050   0.83000   0.83421       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions.label_ids, preds, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e3545dc-5e9e-436c-894e-5284d4b379da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b844130e-3fa3-4a93-a277-13beceab3e4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: labeled_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m: preds, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_label\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions\u001b[38;5;241m.\u001b[39mlabel_ids})\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame({'text': labeled_data['test']['text'], 'prediction': preds, 'true_label': predictions.label_ids}).to_excel('preds.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2b220c8-f45a-48a0-9f7c-0e1eb243f028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fe5c4036700>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(predictions.label_ids, preds)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4afca46f-44ea-4260-a04f-34302be2fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hp_space_ray(trial):\n",
    "    from ray import tune\n",
    "\n",
    "    return {\n",
    "        \"learning_rate\": tune.loguniform(1e-5, 5e-5),\n",
    "        \"per_gpu_batch_size\": tune.choice([4, 8, 16, 32, 64]),\n",
    "        \"warmup_steps\": tune.choice(range(0, 500)),\n",
    "        \"weight_decay\": tune.loguniform(0.01, 0.3),\n",
    "        \"num_epochs\": tune.choice(range(2, 5)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3d4b5b4-fc55-4595-ad2a-ae6ac068543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f1205c-fca9-4fd0-be6b-c4957dda70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
